{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMdxN7ycEr8LzoRSoA+SwAg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shikhadiwakar/ML_LAB/blob/main/autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Woring of an Auto-Encoder**"
      ],
      "metadata": {
        "id": "F-5F9zTtVgP_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uW_GepRNrON"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import losses\n",
        "from scipy.stats import norm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data â€“ training and test\n",
        "(x_tr, y_tr), (x_te, y_te) = mnist.load_data()"
      ],
      "metadata": {
        "id": "Moa9iULQN4W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization and Reshaping of images (flatten)\n",
        "x_tr, x_te = x_tr.astype('float32')/255., x_te.astype('float32')/255.\n",
        "x_tr_flat, x_te_flat = x_tr.reshape(x_tr.shape[0], -1), x_te.reshape(x_te.shape[0], -1)"
      ],
      "metadata": {
        "id": "Rcy5XDYwN_MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_tr.shape, x_te.shape)\n",
        "print(x_tr_flat.shape, x_te_flat.shape)"
      ],
      "metadata": {
        "id": "OSgjdDL-OEn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Neural Network Parameters\n",
        "batch_size, n_epoch = 100, 50\n",
        "n_hidden, z_dim = 256, 2"
      ],
      "metadata": {
        "id": "UrBy7TVYONBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a training image\n",
        "plt.imshow(x_tr[1]);"
      ],
      "metadata": {
        "id": "XGujLwL0OPJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sampling function\n",
        "def sampling(args):\n",
        "    mu, log_var = args\n",
        "    eps = K.random_normal(shape=(batch_size, z_dim), mean=0., stddev=1.0)\n",
        "    return mu + K.exp(log_var) * eps"
      ],
      "metadata": {
        "id": "837ZTbivOVED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sinc function interpolates the samples, and therefore reconstructs the continuous image from the set of samples."
      ],
      "metadata": {
        "id": "ak8Ks5LTQU2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder - from 784->256->128->2\n",
        "inputs_flat = Input(shape=(x_tr_flat.shape[1:]))\n",
        "x_flat = Dense(n_hidden, activation='relu')(inputs_flat) # first hidden layer\n",
        "x_flat = Dense(n_hidden//2, activation='relu')(x_flat)  # second hidden layer\n",
        "\n",
        "# hidden state, which we will pass into the Model to get the Encoder.\n",
        "mu_flat = Dense(z_dim)(x_flat)\n",
        "log_var_flat = Dense(z_dim)(x_flat)\n",
        "z_flat = Lambda(sampling, output_shape=(z_dim,))([mu_flat, log_var_flat])"
      ],
      "metadata": {
        "id": "Ron9H20TQYHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Compresses the input data into an encoded representation. The encoder layer compresses the input image into a latent space representation."
      ],
      "metadata": {
        "id": "Lbxw6LNNQ6W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder - from 2->128->256->784\n",
        "latent_inputs = Input(shape=(z_dim,))\n",
        "z_decoder1 = Dense(n_hidden//2, activation='relu')\n",
        "z_decoder2 = Dense(n_hidden, activation='relu')\n",
        "y_decoder = Dense(x_tr_flat.shape[1], activation='sigmoid')\n",
        "z_decoded = z_decoder1(latent_inputs)\n",
        "z_decoded = z_decoder2(z_decoded)\n",
        "y_decoded = y_decoder(z_decoded)\n",
        "decoder_flat = Model(latent_inputs, y_decoded, name=\"decoder_conv\")\n",
        "\n",
        "outputs_flat = decoder_flat(z_flat)"
      ],
      "metadata": {
        "id": "mjShnZYCQfUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder Converts the two-dimensional vector into the output sequence. The decoder converts the hidden vector into the output sequence."
      ],
      "metadata": {
        "id": "RJKBKouyQ-mO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***VAEs are generative models that capture the probability distribution of a dataset and generate new samples. ***\n",
        "\n",
        "They have an encoder-decoder structure:\n",
        "\n",
        "Encoder: Takes in data and transforms it into a latent representation\n",
        "\n",
        "Decoder: Takes a latent representation and returns a reconstruction\n",
        "\n",
        "VAEs optimize a lower bound of the data likelihood. This requires the discovery of q-distributions, or variational posteriors."
      ],
      "metadata": {
        "id": "E6T4A-YrRVZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# variational autoencoder (VAE) - to reconstruction input\n",
        "reconstruction_loss = losses.binary_crossentropy(inputs_flat,\n",
        "                                                 outputs_flat) * x_tr_flat.shape[1]\n",
        "kl_loss = 0.5 * K.sum(K.square(mu_flat) + K.exp(log_var_flat) - log_var_flat - 1, axis = -1)\n",
        "vae_flat_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "# Build model\n",
        "#  Ensure that the reconstructed outputs are as close to the inputs\n",
        "vae_flat = Model(inputs_flat, outputs_flat)\n",
        "vae_flat.add_loss(vae_flat_loss)\n",
        "vae_flat.compile(optimizer='adam')"
      ],
      "metadata": {
        "id": "HKLZyArURCS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "vae_flat.fit(\n",
        "    x_tr_flat,\n",
        "    shuffle=True,\n",
        "    epochs=n_epoch,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(x_te_flat, None),\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "CKtVj4GERs1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Visualize Embeddings**"
      ],
      "metadata": {
        "id": "3L5l4ZBvR-WP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build encoders\n",
        "encoder_f = Model(inputs_flat, z_flat)  # flat encoder"
      ],
      "metadata": {
        "id": "5T5e9f8eSD2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot of the digit classes in the latent space\n",
        "x_te_latent = encoder_f.predict(x_te_flat, batch_size=batch_size,verbose=0)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(x_te_latent[:, 0], x_te_latent[:, 1], c=y_te, alpha=0.75)\n",
        "plt.title('MNIST 2D Embeddings')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qYZL3ON1SILM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "APPLICATION"
      ],
      "metadata": {
        "id": "4XqDrqOdUCMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "UDs3_wsoUJbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "oI4Bq_7pT6yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentences we want to encode. Example:\n",
        "sentence = ['The team enjoyed the hike through the meadow',\n",
        "            'The national park had great views',\n",
        "            'Olive oil drizzled over pizza tastes delicious']"
      ],
      "metadata": {
        "id": "YNqHEneMUk-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentences are encoded by calling model.encode()\n",
        "embedding = model.encode(sentence)\n",
        "\n",
        "#Preview the embeddings\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "FgUcXV35Un6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding.shape"
      ],
      "metadata": {
        "id": "v-Lo4iGYUsGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.heatmap(embedding[0].reshape(-1,384),cmap=\"Greys\",center=0,square=False)\n",
        "plt.gcf().set_size_inches(10,1)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "sns.heatmap(embedding[1].reshape(-1,384),cmap=\"Greys\",center=0,square=False)\n",
        "plt.gcf().set_size_inches(10,1)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "sns.heatmap(embedding[2].reshape(-1,384),cmap=\"Greys\",center=0,square=False)\n",
        "plt.gcf().set_size_inches(10,1)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZlGHmPO2UteS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dot Product\n",
        "print(\"Distance 0-1:\", np.dot(embedding[0], embedding[1]))\n",
        "print(\"Distance 0-2:\", np.dot(embedding[0], embedding[2]))\n",
        "print(\"Distance 1-2:\", np.dot(embedding[1], embedding[2]))"
      ],
      "metadata": {
        "id": "1uy-EMo1U2C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cosine Distance function\n",
        "def cosine_distance(vec1,vec2):\n",
        "  cosine = 1 - (np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2)))\n",
        "  return cosine"
      ],
      "metadata": {
        "id": "0TGn8vZtVPGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cosine Distance\n",
        "print(\"Distance 0-1: \", cosine_distance(embedding[0], embedding[1]))\n",
        "print(\"Distance 0-2: \", cosine_distance(embedding[0], embedding[2]))\n",
        "print(\"Distance 1-2: \", cosine_distance(embedding[1], embedding[2]))"
      ],
      "metadata": {
        "id": "ExoWvrfGVCHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}